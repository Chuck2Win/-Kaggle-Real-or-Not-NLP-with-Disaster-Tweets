{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data의 text를 통해서 disaster관련 문장이면 target 1 아니면 0\n",
    "\n",
    "initial method\n",
    "\n",
    "text -> tokenized \n",
    "\n",
    "using pre trained word embedding (Word2Vec)\n",
    "\n",
    "GRU (bidirectional)\n",
    "\n",
    "result\n",
    "\n",
    "input data : text\n",
    "\n",
    "output data : target(digit 0,1)\n",
    "\n",
    "model : architecture (GRU) // params : (embedding,GRU의 params)\n",
    "\n",
    "cost : binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\IDSL\\\\Desktop\\\\파이썬공부\\\\-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2\\\\-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('./train.csv',header=0,index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "1     our deeds are the reason of this earthquake ma...\n",
      "4               forest fire near la ronge sask. canada \n",
      "5     all residents asked to shelter in place are be...\n",
      "6     , people receive wildfires evacuation orders i...\n",
      "7     just got sent this photo from ruby alaska as s...\n",
      "8     rockyfire update california hwy. closed in bot...\n",
      "10    flood disaster heavy rain causes flash floodin...\n",
      "13    im on top of the hill and i can see a fire in ...\n",
      "14    theres an emergency evacuation happening now i...\n",
      "15    im afraid that the tornado is coming to our ar...\n",
      "16         three people died from the heat wave so far \n",
      "17    haha south tampa is getting flooded hah wait a...\n",
      "18    raining flooding florida tampabay tampa or day...\n",
      "19               flood in bago myanmar we arrived bago \n",
      "20    damage to school bus on in multi car crash bre...\n",
      "23                                        whats up man \n",
      "24                                       i love fruits \n",
      "25                                    summer is lovely \n",
      "26                                   my car is so fast \n",
      "28                              what a goooooooaaaaaal \n",
      "31                              this is ridiculous.... \n",
      "32                                      london is cool \n",
      "33                                         love skiing \n",
      "34                                what a wonderful day \n",
      "36                                            looooool \n",
      "37                       no way...i cant eat that shit \n",
      "38                                was in nyc last week \n",
      "39                                  love my girlfriend \n",
      "40                                              cooool \n",
      "41                                   do you like pasta \n",
      "Name: post_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1. text data pre processing\n",
    "# cleanse, tokenize, encoding, vectorize\n",
    "\n",
    "# 1.1. cleanse\n",
    "import re\n",
    "# 이전엔 그냥 str의 replace 활용했는데 #는 못바꾸넹..\n",
    "# 대문자 -> 소문자\n",
    "# 특수문자 제거 (구두점, 쉼표 제외)\n",
    "# http 부분 제거 -> split으로 한 다음에 http 가 있다면 그 부분을 지우고 다시 합치자\n",
    "def cleanse(sentence):\n",
    "    result=sentence.lower()\n",
    "    result=re.sub('[^a-z ,.]','',result)\n",
    "    result=[i for i in result.split() if 'http' not in i]\n",
    "    Result=''\n",
    "    for i in result:\n",
    "        Result=Result+i+' '\n",
    "    return Result\n",
    "data['post_text']=data['text'].apply(lambda i : cleanse(i))\n",
    "print(data.post_text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # of data\n",
    "len(data) #7613\n",
    "# train_dev split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data,val_data=train_test_split(data,test_size=613,shuffle=True) # train data 7000, val data 613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\IDSL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk로 tokenize\n",
    "# 1.2. tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# train_text_tokenized=word_tokenize(train_data.post_text) #-> error 발생 -> 애초에 word_tokenize는 str을 위한 것\n",
    "\n",
    "# http가 들어간 sentence들이 있어서 그 부분은 제거해주자.\n",
    "def word_tokenize_1(sentence):\n",
    "    j=word_tokenize(sentence)\n",
    "    result=[i for i in j if 'http' not in i] \n",
    "    return result\n",
    "sentence_tokenized=train_data['post_text'].apply(lambda i : word_tokenize_1(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3037    [usgs, eq, m, ., km, ssw, of, anza, california...\n",
      "5822    [chinas, stock, market, crash, are, there, gem...\n",
      "6985    [the, sharper, image, viper, hardside, twister...\n",
      "5893    [socialwots, globiinclusion, rt, nrcmiddleeast...\n",
      "3690    [i, liked, a, youtube, video, from, vgbootcamp...\n",
      "Name: post_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sentence_tokenized.head()) # http가 들어있는 부분들이 있는데 이는 제거하자.-> 제거 完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 word embedding -> using pre trained word2vec -> 너무 커서 일단은 pretrained된 것을 활용 O -> 구글\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "# KeyedVectors는 학습이 안된다...\n",
    "# word2vec=KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "#  일단 training data만으로 word2vec을 학습시켜서 진행을 하자.(계산의 편의를 위해)\n",
    "word2vec_toy=Word2Vec(sentence_tokenized,size=300,min_count=3,window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.1 wordembedding -> pytorch & word encoding \n",
    "# 직접 일일히 짜지 말고 torchtext를 활용하자\n",
    "import torchtext\n",
    "word2vec_toy_vocab=tuple(word2vec_toy.wv.vocab.keys())\n",
    "word2index={word:index+2 for index,word in enumerate(word2vec_toy_vocab)} # 0 for padding 1 for unk\n",
    "index2word={index+2:word for index,word in enumerate(word2vec_toy_vocab)} # 0 for padding 1 for unk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 preprocessing\n",
    "# torchtext를 활용하면 한방에(tokenize,wordembedding,padding ...)\n",
    "# SOS,EOS,padding,fix_len : 40 (애초에 twitter는 글자수 제한이 280)\n",
    "\n",
    "# Field 정의\n",
    "Text=torchtext.data.Field(sequential=True,use_vocab=True, init_token='<SOS>',eos_token='<EOS>',tokenize=word_tokenize,batch_first=True,lower=True,fix_length=40)\n",
    "Target=torchtext.data.Field(sequential=False,use_vocab=False,batch_first=True,is_target=True)\n",
    "Id=torchtext.data.Field(sequential=False,use_vocab=False,batch_first=True)\n",
    "\n",
    "# Data 정의\n",
    "# 영어와 ., 만을 남긴 csv 형태로 넘겨주자\n",
    "# 또한 http부분을 제거\n",
    "train_data[['post_text','target']].to_csv('train_data.csv',index=True)\n",
    "train_data_1=torchtext.data.TabularDataset('./train_data.csv',format='csv',fields=[('id',Id),('text',Text),('target',Target)],\n",
    "                                           skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2454',\n",
       " 'text': ['cyclist',\n",
       "  'who',\n",
       "  'collided',\n",
       "  'with',\n",
       "  'runner',\n",
       "  'on',\n",
       "  'roanoke',\n",
       "  'greenway',\n",
       "  'wins',\n",
       "  'civil',\n",
       "  'verdict',\n",
       "  'via',\n",
       "  'roanoketimes'],\n",
       " 'target': '0'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vars 객체의 내부 변수가 저장된 딕셔너리 반환\n",
    "vars(train_data_1[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary 만들기\n",
    "Text.build_vocab(train_data_1,min_freq=3)\n",
    "# 단어 집합의 크기\n",
    "# print(len(Text.vocab))\n",
    "# 단어 집합의 내부 단어들 확인하기\n",
    "# print(Text.vocab.stoi)\n",
    "# 싹 다 vectorized도 됬다.\n",
    "\n",
    "# 근데 사전 훈련된 Word2Vec을 초기 임베딩으로 활용하기, Word2Vec 토큰화 된 녀석을 받는다\n",
    "#  torchtext.vocab.Vectors(name<-name of files)\n",
    "# https://discuss.pytorch.org/t/aligning-torchtext-vocab-index-to-loaded-embedding-pre-trained-weights/20878/2\n",
    "\n",
    "word2vec_toy=Word2Vec(train_data['post_text'].apply(lambda i : word_tokenize(i))\n",
    "                      ,size=300,min_count=3,window=3)\n",
    "# Word2Vec.wv.vocab.keys()\n",
    "word2vec_toy.wv.save_word2vec_format('./wv',binary=False)\n",
    "# # Text.build_vocab(train_data_1,min_freq=3,vectors=word2vec_toy)\n",
    "# # print(Text.vocab.stoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1a8d72d97f0>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/4092 [00:00<?, ?it/s]Skipping token b'4092' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|███████████████████████████████████████████████████████████████████████████▊| 4081/4092 [00:00<00:00, 5800.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vectors at 0x1a8d73b5240>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.vocab.Vectors('wv',cache='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]Skipping token b'28' with 1-dimensional vector [b'300']; likely a header\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-179-ec5486e7331d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec.model'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36mcache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m    413\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                     \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvectors_loaded\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m                     \u001b[0mvectors_loaded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                     \u001b[0mitos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    413\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                     \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvectors_loaded\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m                     \u001b[0mvectors_loaded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                     \u001b[0mitos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "torchtext.vocab.Vectors('word2vec.model',cache='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,   0,   0,  ...,   1,   1,   1],\n",
      "        [  2, 537, 437,  ...,   1,   1,   1],\n",
      "        [  2, 129,   7,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  2,   0,   0,  ...,   1,   1,   1],\n",
      "        [  2,  45, 114,  ...,   1,   1,   1],\n",
      "        [  2,   0,   0,  ...,   1,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "# Iterator로 data 불러오기\n",
    "batch_size=128\n",
    "train_loader=torchtext.data.Iterator(train_data_1,batch_size=batch_size)\n",
    "print(next(iter(train_loader)).text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
