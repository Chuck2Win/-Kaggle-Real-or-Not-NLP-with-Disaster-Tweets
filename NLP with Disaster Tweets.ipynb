{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data의 text를 통해서 disaster관련 문장이면 target 1 아니면 0\n",
    "\n",
    "initial method\n",
    "\n",
    "text -> tokenized \n",
    "\n",
    "using pre trained word embedding (Word2Vec)\n",
    "\n",
    "- GRU (bidirectional)\n",
    "- 추가로 CNN으로 구성해볼래 (Convolutional Neural Networks for sentence classification)(4.1)\n",
    "\n",
    "result\n",
    "\n",
    "input data : text\n",
    "\n",
    "output data : target(digit 0,1)\n",
    "\n",
    "model : architecture (GRU) // params : (embedding,GRU의 params)\n",
    "\n",
    "cost : binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\IDSL\\\\Desktop\\\\파이썬공부\\\\파이토치\\\\kaggle challenge\\\\-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2\\\\-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('./train.csv',header=0,index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "1     our deeds are the reason of this earthquake ma...\n",
      "4               forest fire near la ronge sask. canada \n",
      "5     all residents asked to shelter in place are be...\n",
      "6     , people receive wildfires evacuation orders i...\n",
      "7     just got sent this photo from ruby alaska as s...\n",
      "8     rockyfire update california hwy. closed in bot...\n",
      "10    flood disaster heavy rain causes flash floodin...\n",
      "13    im on top of the hill and i can see a fire in ...\n",
      "14    theres an emergency evacuation happening now i...\n",
      "15    im afraid that the tornado is coming to our ar...\n",
      "16         three people died from the heat wave so far \n",
      "17    haha south tampa is getting flooded hah wait a...\n",
      "18    raining flooding florida tampabay tampa or day...\n",
      "19               flood in bago myanmar we arrived bago \n",
      "20    damage to school bus on in multi car crash bre...\n",
      "23                                        whats up man \n",
      "24                                       i love fruits \n",
      "25                                    summer is lovely \n",
      "26                                   my car is so fast \n",
      "28                              what a goooooooaaaaaal \n",
      "31                              this is ridiculous.... \n",
      "32                                      london is cool \n",
      "33                                         love skiing \n",
      "34                                what a wonderful day \n",
      "36                                            looooool \n",
      "37                       no way...i cant eat that shit \n",
      "38                                was in nyc last week \n",
      "39                                  love my girlfriend \n",
      "40                                              cooool \n",
      "41                                   do you like pasta \n",
      "Name: post_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1. text data pre processing\n",
    "# cleanse, tokenize, encoding, vectorize\n",
    "\n",
    "# 1.1. cleanse\n",
    "import re\n",
    "# 이전엔 그냥 str의 replace 활용했는데 #는 못바꾸넹..\n",
    "# 대문자 -> 소문자\n",
    "# 특수문자 제거 (구두점, 쉼표 제외)\n",
    "# http 부분 제거 -> split으로 한 다음에 http 가 있다면 그 부분을 지우고 다시 합치자\n",
    "def cleanse(sentence):\n",
    "    result=sentence.lower()\n",
    "    result=re.sub('[^a-z ,.]','',result)\n",
    "    result=[i for i in result.split() if 'http' not in i]\n",
    "    Result=''\n",
    "    for i in result:\n",
    "        Result=Result+i+' '\n",
    "    return Result\n",
    "data['post_text']=data['text'].apply(lambda i : cleanse(i))\n",
    "print(data.post_text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # of data\n",
    "len(data) #7613\n",
    "# train_dev split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data,val_data=train_test_split(data,test_size=613,shuffle=True) # train data 7000, val data 613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\IDSL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk로 tokenize\n",
    "# 1.2. tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# train_text_tokenized=word_tokenize(train_data.post_text) #-> error 발생 -> 애초에 word_tokenize는 str을 위한 것\n",
    "\n",
    "# http가 들어간 sentence들이 있어서 그 부분은 제거해주자.\n",
    "def word_tokenize_1(sentence):\n",
    "    j=word_tokenize(sentence)\n",
    "    result=[i for i in j if 'http' not in i] \n",
    "    return result\n",
    "sentence_tokenized=train_data['post_text'].apply(lambda i : word_tokenize_1(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "9830     [a, i, started, writing, when, i, couldnt, tal...\n",
      "5720        [hornybigbadwolf, sets, the, forest, on, fire]\n",
      "8098     [tennoatax, i, hand, you, a, glass, of, water,...\n",
      "10813    [ohhmyjoshh, stevenrulles, he, not, gon, na, b...\n",
      "5207     [.kurtschlichter, yep, considering, that, mill...\n",
      "Name: post_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sentence_tokenized.head()) # http가 들어있는 부분들이 있는데 이는 제거하자.-> 제거 完"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 preprocessing\n",
    "# torchtext를 활용하면 한방에(tokenize,wordembedding,padding ...)\n",
    "# SOS,EOS,padding,fix_len : 40 (애초에 twitter는 글자수 제한이 280)\n",
    "import torchtext\n",
    "# Field 정의\n",
    "# Text <- padding 하지 말아라\n",
    "Text=torchtext.data.Field(sequential=True,use_vocab=True,tokenize=word_tokenize_1,batch_first=True,lower=True,fix_length=None,)\n",
    "Target=torchtext.data.Field(sequential=False,use_vocab=False,batch_first=True,is_target=True)\n",
    "Id=torchtext.data.Field(sequential=False,use_vocab=False,batch_first=True)\n",
    "\n",
    "# Data 정의\n",
    "# 영어와 ., 만을 남긴 csv 형태로 넘겨주자\n",
    "# 또한 http부분을 제거\n",
    "train_data[['post_text','target']].to_csv('train_data.csv',index=True)\n",
    "train_data_1=torchtext.data.TabularDataset('./train_data.csv',format='csv',fields=[('id',Id),('text',Text),('target',Target)],\n",
    "                                           skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '19',\n",
       " 'text': ['flood', 'in', 'bago', 'myanmar', 'we', 'arrived', 'bago'],\n",
       " 'target': '1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vars 객체의 내부 변수가 저장된 딕셔너리 반환\n",
    "vars(train_data_1[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Word2Vec(sentence_tokenized,size=300,window=3,min_count=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4098\n"
     ]
    }
   ],
   "source": [
    "print(len(model2.wv.vocab.keys())) # 4098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4098\n",
      "4098\n",
      "[('disasters', 0.7751655578613281), ('calamity', 0.740983784198761), ('catastrophe', 0.7316239476203918), ('tragedy', 0.5871202945709229), ('devastation', 0.5772956013679504), ('earthquake', 0.5394471883773804), ('tsunami', 0.5323168635368347), ('floods', 0.5304960012435913), ('catastrophic', 0.5219818353652954), ('flood', 0.5129648447036743)]\n",
      "(4098, 300)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights\n",
    "# pretrained model을 활용하는데\n",
    "# 하고자 하는 수순\n",
    "# 1. 일단 model을 구축한다(train은 하지 말고, 일단 vocab만 만들어 둔다)\n",
    "# 2. pretrained model을 가져와서 기존의 model과 intersect를 진행 (기존 model의 vocab만 활용, 그리고 pretrained model의 weight만 활용)\n",
    "# 3. model 학습\n",
    "\n",
    "#1\n",
    "model = Word2Vec(size=300,window=3,min_count=3)\n",
    "model.build_vocab(sentence_tokenized)\n",
    "print(len(model.wv.vocab)) # 4098\n",
    "\n",
    "#2\n",
    "model.intersect_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "print(len(model.wv.vocab))\n",
    "\n",
    "print(model.wv.most_similar('disaster'))\n",
    "print(model.wv.vectors.shape)\n",
    "\n",
    "#3\n",
    "# If sentences is the same corpus that was provided to build_vocab() earlier, \n",
    "# you can simply use total_examples=self.corpus_count.\n",
    "model.train(sentence_tokenized,total_examples=model.corpus_count,epochs=500)\n",
    "print(model.wv.most_similar('disaster'))\n",
    "print(model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim.models.Word2Vec.intersect_word2vec_format\n",
    "Merge the input-hidden weight matrix from the original C word2vec-tool format given, where it intersects with the current vocabulary. (No words are added to the existing vocabulary, but intersecting words adopt the file’s weights, and non-intersecting words are left alone.)\n",
    "binary is a boolean indicating whether the data is in binary word2vec format.\n",
    "-> google 같은 경우엔 binary true로 해야겠지.\n",
    "lockf is a lock-factor value to be set for any imported word-vectors; the default value of 0.0 prevents further updating of the vector during subsequent training. Use 1.0 to allow further training updates of merged vectors.\n",
    "\n",
    "cf)\n",
    "https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-14%EC%9D%BC%EC%B0%A8-word2vec-%EC%8B%A4%EC%8A%B52-8e518a358b6c\n",
    "\n",
    "- model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "https://m.blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221349385092&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext로 구성해낸다.\n",
    "# 관건 align 작업\n",
    "model.wv.save_word2vec_format('C:/Users/IDSL/Desktop/파이썬공부/파이토치/kaggle challenge/-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2/-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2/wv')\n",
    "vectors=torchtext.vocab.Vectors('wv',cache='C:/Users/IDSL/Desktop/파이썬공부/파이토치/kaggle challenge/-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2/-Kaggle-Real-or-Not-NLP-with-Disaster-Tweets-day2/') #'./wv'모델을 읽어온다.\n",
    "# 안될 때엔 변수 싹 다 지우고 다시 해라...\n",
    "# 이 짓 너무 했다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word==list(vectors.stoi.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/aligning-torchtext-vocab-index-to-loaded-embedding-pre-trained-weights/20878/2\n",
    " # load embeddings using torchtext\n",
    "Text.build_vocab(train_data_1,min_freq=3,vectors=vectors)\n",
    "# Text.vocab.set_vectors(vectors.stoi,vectors.vectors,vectors.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 다 같은 것을 입증.\n",
    "# freq가 달라도, vectors에 있던 것만 들어가게 된다.\n",
    "result=[]\n",
    "for vocab,_ in vectors.stoi.items():\n",
    "    r=vectors.vectors[_]==Text.vocab.vectors[Text.vocab.stoi[vocab]]\n",
    "    result.append(np.all(r.numpy()))\n",
    "print(np.all(np.array(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "train_loader=torchtext.data.Iterator(train_data_1,batch_size=batch_size)\n",
    "batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   9,  213,    0,  669,    6,  148,   28, 2415,   28,  560,    5,  889,\n",
      "            7,   15,  814,    8,    0,  527,    0, 2809,    0,   44],\n",
      "        [ 414,   24,   19,    0,  531,  481,   10,    0,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(batch.text)\n",
    "print(batch.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "eb=nn.Embedding(4100,300).from_pretrained(Text.vocab.vectors)\n",
    "f=nn.Sequential(nn.Conv1d(300,100,2,1,1),nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=eb(batch.text).transpose(1,2)\n",
    "z=f(e)\n",
    "z.max(dim=2)[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_sentiment(nn.Module):\n",
    "    def __init__(self,embedding_dim,num_of_filter):\n",
    "        super().__init__()\n",
    "        self.embedd=nn.Embedding(Text.vocab.vectors.shape[0],embedding_dim).from_pretrained(Text.vocab.vectors)\n",
    "        self.conv1=nn.Sequential(nn.Conv1d(embedding_dim,100,2,1,1),nn.ReLU())\n",
    "        self.conv2=nn.Sequential(nn.Conv1d(embedding_dim,100,3,1,1),nn.ReLU())\n",
    "        self.conv3=nn.Sequential(nn.Conv1d(embedding_dim,100,4,1,1),nn.ReLU())\n",
    "        self.fc=nn.Sequential(nn.dropout(0.5),nn.Linear(300,2),nn.ReLU())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
